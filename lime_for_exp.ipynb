{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験用にLIMEで画像を生成、保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import timm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "import timm.models.mlp_mixer\n",
    "import numpy as np\n",
    "import exchange_tensor_array as exchange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.utils import download_url\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import glob\n",
    "\n",
    "import time\n",
    "\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import slackweb\n",
    "slack = slackweb.Slack(\"https://hooks.slack.com/services/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\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jpgファイルとxmlファイルを取得する（experience_1.ipynbと同じ処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_files = glob.glob(\"xmls/*\")\n",
    "file_names = []\n",
    "for file in xml_files:\n",
    "    file_names.append(file)\n",
    "\n",
    "file_date = []\n",
    "for file_name in file_names:\n",
    "    xml_file = open(file_name)\n",
    "    xmll_tree = ET.parse(xml_file)\n",
    "    root = xmll_tree.getroot()\n",
    "    for obj in root.iter(\"size\"):\n",
    "        h = int(obj.find(\"height\").text)\n",
    "        w = int(obj.find(\"width\").text)\n",
    "    jpg_name = root[1].text\n",
    "    if min(h, w) < 256 or max(h, w) / min(h, w) >= 1.025:\n",
    "        continue\n",
    "    if jpg_name[-3:] != \"jpg\":\n",
    "        continue\n",
    "    img = Image.open(\"pet_dataset/\" + jpg_name)\n",
    "    if img.mode != \"RGB\":\n",
    "        continue\n",
    "    file_date.append([max(h, w) / min(h, w), jpg_name, file_name])\n",
    "ok_files_name = [i[1:] for i in sorted(file_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIMEの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル作成\n",
    "model = timm.create_model(\"gmlp_s16_224\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "#lime用の関数\n",
    "def get_pil_transform(): \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224)\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_preprocess_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])     \n",
    "    transf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf    \n",
    "\n",
    "pill_transf = get_pil_transform()\n",
    "preprocess_transform = get_preprocess_transform()\n",
    "\n",
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()\n",
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価実験1の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Experience_bounding_box():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        初期化\n",
    "        \"\"\"\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),  # (256, 256) で切り抜く。\n",
    "                transforms.CenterCrop(224),  # 画像の中心に合わせて、(224, 224) で切り抜く\n",
    "            ]\n",
    "        )\n",
    "\n",
    " \n",
    "    def open_and_resize_original_image(self, img_path : str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        img_pathの元画像を読み込んでサイズを(244, 244)にして返す\n",
    "        \"\"\"\n",
    "        original_image = Image.open(img_path)\n",
    "        original_image = self.transform(original_image)\n",
    "        \n",
    "        return original_image\n",
    "    \n",
    "    \n",
    "    def open_result_image(self, img_path : str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        img_pathのマッピングされた画像を読み込んで返す（サイズの変更はしない）\n",
    "        \"\"\"\n",
    "        result_image = Image.open(img_path)\n",
    "        \n",
    "        return result_image\n",
    "    \n",
    "    \n",
    "    def get_image_size_and_bounding_box_cornerpoints(self, file_path : str) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        file_pathのxmlファイルから画像サイズとバウンディングボックスの座標を読み込んで返す\n",
    "        [(height, width), (ymin, ymax, xmax, xmin)]\n",
    "        \"\"\"\n",
    "        xml_file = open(file_path)\n",
    "        xmll_tree = ET.parse(xml_file)\n",
    "        root = xmll_tree.getroot()\n",
    "        for obj in root.iter(\"size\"):\n",
    "            h = int(obj.find(\"height\").text)\n",
    "            w = int(obj.find(\"width\").text)\n",
    "        \n",
    "        for obj in root.iter('object'):\n",
    "            xmlbox = obj.find(\"bndbox\")\n",
    "            ymin = int(xmlbox.find('ymin').text)\n",
    "            ymax = int(xmlbox.find('ymax').text)\n",
    "            xmin = int(xmlbox.find('xmin').text)\n",
    "            xmax = int(xmlbox.find('xmax').text)\n",
    "        \n",
    "        \n",
    "        return [(h, w), (ymin, ymax, xmin, xmax)]\n",
    "            \n",
    "    \n",
    "       \n",
    "    def get_resized_box_corner(self, original_size : tuple, original_corner_points : tuple) -> list:\n",
    "        \"\"\"\n",
    "        original_cornerで表されるバウンディングボックスを持つ、サイズがoriginal_size(y, x)の画像が(224, 224)にresizeした後の、\n",
    "        バウンディングボックスのの座標を返す\n",
    "        [ymin, ymax, xmax, xmin]\n",
    "        \"\"\"\n",
    "        h, w = original_size\n",
    "        ymin, ymax, xmin, xmax = original_corner_points\n",
    "        \n",
    "        # xmlファイルの座標は右下原点(?)なので上下左右に反転させて左上原点に直す\n",
    "        # ymin = h - ymin - 1\n",
    "        # ymax = h - ymax - 1\n",
    "        # xmin = w - xmin - 1\n",
    "        # xmax = w - xmax - 1\n",
    "        \n",
    "        # transforms.Resize(256)に相当する座標変換を行う\n",
    "        tmp = min(h, w)\n",
    "        ymin = ymin * 256 // tmp\n",
    "        ymax = ymax * 256 // tmp\n",
    "        xmin = xmin * 256 // tmp\n",
    "        xmax = xmax * 256 // tmp\n",
    "        \n",
    "        # transforms.CenterCrop(224)に相当する座標変換を行う\n",
    "        tmp = (256 - 224) // 2\n",
    "        ymin -= tmp\n",
    "        ymax -= tmp\n",
    "        xmin -= tmp\n",
    "        xmax -= tmp\n",
    "        \n",
    "        return [ymin, ymax, xmin, xmax]\n",
    "    \n",
    "    \n",
    "    def cal_val(self, original_img_path : str, result_img_path : str, xml_path : str) -> list[tuple]:\n",
    "        \"\"\"元画像とマッピングされた画像を読み込んで画素値を比較してマッピングされてるかを判定する方式。上手く働かない（他の箇所も画素値が変わってしまっているため）\"\"\"\n",
    "        # 元画像の読み込みと処理\n",
    "        original_img = self.open_and_resize_original_image(original_img_path)\n",
    "        # マッピングされた画像の読み込み\n",
    "        result_img = self.open_result_image(result_img_path)\n",
    "        # xmlファイルから元画像のサイズと4つの座標の読み込み\n",
    "        original_img_size, bndbox_coordinates = self.get_image_size_and_bounding_box_cornerpoints(xml_path)\n",
    "        # 返還後のバウンディングボックスの4つの頂点の座標の算出\n",
    "        ymin, ymax, xmin, xmax = self.get_resized_box_corner(original_img_size, bndbox_coordinates)\n",
    "        print(ymin, ymax, xmin, xmax)\n",
    "        \n",
    "        count_in_bndbox = [0, 0] # バウンディングボックスの中のマッピングされた割合を記録するためのリスト. 0でマッピングされている, 1でマッピングされていない\n",
    "        count_mapped = [0, 0] # マッピングされた領域のうち、バウンディングボックスの内外にある割合を記録するためのリスト. 0でバウンディングボックスの内側（境界含む）, 1でバウンディングボックスの外側\n",
    "        \n",
    "        mapped_count = 0\n",
    "        bndcount = 0\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                #print(original_img.getpixel((i, j)) , result_img.getpixel((i, j)), original_img.getpixel((i, j)) != result_img.getpixel((i, j)))\n",
    "                if ymin <=   i <= ymax and xmin <= j <= xmax:\n",
    "                    bndcount += 1\n",
    "                if original_img.getpixel((i, j)) != result_img.getpixel((i, j)): # マッピングされている\n",
    "                    mapped_count += 1\n",
    "                    if ymin <= i <= ymax and xmin <= j <= xmax: # バウンディングボックスの内側\n",
    "                        count_in_bndbox[0] += 1\n",
    "                        #print(i, j)\n",
    "                        count_mapped[0] += 1\n",
    "                    else:# バウンディングボックスの外側\n",
    "                        count_mapped[1] += 1\n",
    "                elif ymin <= i <= ymax and xmin <= j <= xmax: # マッピングされていないかつバウンディングボックスの内側\n",
    "                    count_in_bndbox[1] += 1\n",
    "        \n",
    "        a, b = count_in_bndbox\n",
    "        count_in_bndbox.append(a * 100 / (a + b))\n",
    "        a, b = count_mapped\n",
    "        count_mapped.append(a * 100 / (a + b))\n",
    "        print(mapped_count)\n",
    "        print(bndcount)\n",
    "        \n",
    "        return[tuple(count_in_bndbox), tuple(count_mapped)]\n",
    "    \n",
    "    def cal_val_from_mappingdate(self, masks : dict, shap : list, max_rate : float, xml_path : str) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        マッピングした座標のデータ(caのmasks)から算出する\n",
    "        \"\"\"\n",
    "        # xmlファイルから元画像のサイズと4つの座標の読み込み\n",
    "        original_img_size, bndbox_coordinates = self.get_image_size_and_bounding_box_cornerpoints(xml_path)\n",
    "        # 返還後のバウンディングボックスの4つの頂点の座標の算出\n",
    "        ymin, ymax, xmin, xmax = self.get_resized_box_corner(original_img_size, bndbox_coordinates)\n",
    "        \n",
    "        count_in_bndbox = [0, (ymax - ymin + 1) * (xmax - xmin + 1)] # バウンディングボックスの中のマッピングされた割合を記録するためのリスト. 0でマッピングされている, 1でマッピングされていない\n",
    "        count_mapped = [0, 0] # マッピングされた領域のうち、バウンディングボックスの内外にある割合を記録するためのリスト. 0でバウンディングボックスの内側（境界含む）, 1でバウンディングボックスの外側\n",
    "\n",
    "        border = max(shap) * max_rate\n",
    "        for i in range(len(shap)):\n",
    "            if shap[i] < border:\n",
    "                continue\n",
    "            for y, x in masks[i]: #マッピングされている\n",
    "                if ymin <= y <= ymax and xmin <= x <= xmax: #バウンティボックスの内側\n",
    "                    count_in_bndbox[0] += 1\n",
    "                    count_in_bndbox[1] -= 1\n",
    "                    count_mapped[0] += 1\n",
    "                else:\n",
    "                    count_mapped[1] += 1\n",
    "        a, b = count_in_bndbox\n",
    "        count_in_bndbox.append(a * 100 / (a + b))\n",
    "        a, b = count_mapped\n",
    "        count_mapped.append(a * 100 / (a + b))\n",
    "        \n",
    "        return[tuple(count_in_bndbox), tuple(count_mapped)]\n",
    "    \n",
    "    def cal_val_from_mappedarray(self, mapped_array : list, xml_path : str) -> list[list]:\n",
    "        \"\"\"マッピングされているかをbool値で表す2次元リストから算出する\"\"\"\n",
    "        # xmlファイルから元画像のサイズと4つの座標の読み込み\n",
    "        original_img_size, bndbox_coordinates = self.get_image_size_and_bounding_box_cornerpoints(xml_path)\n",
    "        # 変換後のバウンディングボックスの4つの頂点の座標の算出\n",
    "        ymin, ymax, xmin, xmax = self.get_resized_box_corner(original_img_size, bndbox_coordinates)\n",
    "        count_in_bndbox = [0, (ymax - ymin + 1) * (xmax - xmin + 1)] # バウンディングボックスの中のマッピングされた割合を記録するためのリスト. [0]がマッピングされている, [1]がマッピングされていない\n",
    "        count_mapped = [0, 0] # マッピングされた領域のうち、バウンディングボックスの内外にある割合を記録するためのリスト. [0]がバウンディングボックスの内側（境界含む）, [1]がバウンディングボックスの外側\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                if mapped_array[i][j]: #マッピングされている\n",
    "                    if ymin <= i <= ymax and xmin <= j <= xmax: #バウンディングボックスの内側\n",
    "                        count_in_bndbox[0] += 1\n",
    "                        count_in_bndbox[1] -= 1\n",
    "                        count_mapped[0] += 1\n",
    "                    else:\n",
    "                        count_mapped[1] += 1\n",
    "        a, b = count_in_bndbox\n",
    "        count_in_bndbox.append(a * 100 / (a + b))\n",
    "        c, d = count_mapped\n",
    "        count_mapped.append(c * 100 / (c + d))\n",
    "        \n",
    "        return [count_in_bndbox, count_mapped]\n",
    "    \n",
    "    def cal_val_from_lime_result(self, lime_res : np.ndarray, xml_path : str) -> list[list]:\n",
    "        \"\"\"LIMEの結果(np.array)から算出する\"\"\"\n",
    "        # xmlファイルから元画像のサイズと4つの座標の読み込み\n",
    "        original_img_size, bndbox_coordinates = self.get_image_size_and_bounding_box_cornerpoints(xml_path)\n",
    "        \n",
    "        # 変換後のバウンディングボックスの4つの頂点の座標の算出\n",
    "        ymin, ymax, xmin, xmax = self.get_resized_box_corner(original_img_size, bndbox_coordinates)\n",
    "        \n",
    "        count_in_bndbox = [0, (ymax - ymin + 1) * (xmax - xmin + 1)] # バウンディングボックスの中のマッピングされた割合を記録するためのリスト. [0]がマッピングされている, [1]がマッピングされていない\n",
    "        count_mapped = [0, 0] # マッピングされた領域のうち、バウンディングボックスの内外にある割合を記録するためのリスト. [0]がバウンディングボックスの内側（境界含む）, [1]がバウンディングボックスの外側\n",
    "        \n",
    "        a = np.array([0, 0, 0]) #比較対象用配列\n",
    "        \n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                if np.array_equal(lime_res[i, j], a) == False: #マッピングされている\n",
    "                    if ymin <= i <= ymax and xmin <= j <= xmax: #バウンディングボックスの内側\n",
    "                        count_in_bndbox[0] += 1\n",
    "                        count_in_bndbox[1] -= 1\n",
    "                        count_mapped[0] += 1\n",
    "                    else:\n",
    "                        count_mapped[1] += 1\n",
    "        a, b = count_in_bndbox\n",
    "        count_in_bndbox.append(a* 100 / (a + b))\n",
    "        c, d = count_mapped\n",
    "        count_mapped.append(c * 100 / (c + d))\n",
    "        \n",
    "        return [count_in_bndbox, count_mapped]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIMEを実行して結果の画像を保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_rest == True ver\n",
    "for jpg_name, xml_name in ok_files_name:\n",
    "    try:\n",
    "        input_path = \"pet_dataset/\" + jpg_name\n",
    "        img = Image.open(input_path)\n",
    "        explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
    "                                                batch_predict, # classification function\n",
    "                                                top_labels=1, \n",
    "                                                hide_color=0, \n",
    "                                                num_samples=1000) # number of images that will be sent to classification function\n",
    "        temp1, mask1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=11, hide_rest=True)\n",
    "        img_boundry1 = mark_boundaries(temp1/255.0, mask1)\n",
    "        plt.imsave(\"reslut_pet_dataset/exp1/lime/not_hide_rest/\" + jpg_name, img_boundry1)\n",
    "    except Exception as e:\n",
    "        slack.notify(text=\"えらー\")\n",
    "slack.notify(text=\"しゅーりょー\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIMEを実行して評価実験1(バウンディングボックス)を実行して配列に保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d06b8a0be664cc9ad73af63638f2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1354808693ee4d6998056eda4dcbbd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299278e7857348aa869fef491d318ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4ad72309cd4d08a06c6c98c3415ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af452f4461941e8b9e00e772d9a8483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f356b51ea4f448a7bd02bcaf9eea89af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c1f99df1304ce99d1f3991191639da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181c84533ab84a72897b54ad7f4c3898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cf1b30b7194dc8a79ec62c51fd968c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2528c6f7cf47b5ba9b948245be95ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a69b83f3984ef4a3ecf2eff52aee5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f4a88843441d48e3eb76b974f1ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224f9ee445bc44c6b2c279f022273dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7565b014811743a084a11072cd8c1ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb6e3fa5d564dc9a3301ef206fccded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2630c1273a042f391292cdb83de2a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56d036624eb4dd195c0a98e42ba0a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32caf16bb511434189d38cc364cdfdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34b596731d64386babf8528b040dc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5846f77d60042fabe50bbad549488f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47f00e15a664427a44e7ca369661ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d1b195d967486b85ceb9b3c05dba05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54bd9fa836c4b90a1893582aead34a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90072162ddb4439d8579557a1c61527e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f955db820654196bddfe10b8fac0d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cbbe073a014397985d22a2216ea08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1c35c43b774ca1bde341ee29f872ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786730b694ef4db4af89746608d5f4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523e31bc8ed94512807beb7ded0109d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9340906260462b8d6ac981401f3c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa4e81ca1524455ac8b1e58f0d4dabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8cfbc800340a795c396713b5bf737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "# hide_rest == True ver\n",
    "import time\n",
    "exp1 = Experience_bounding_box()\n",
    "exp1_res = []\n",
    "count = 0\n",
    "\n",
    "with open(\"ok_files_list.txt\", mode=\"r\") as f:\n",
    "    ok_files_name = [ s.strip().split() for s in f.readlines()]\n",
    "\n",
    "t1 = time.time()\n",
    "for jpg_path, xml_path, class_name, prob in ok_files_name:\n",
    "    try:\n",
    "        img = Image.open(jpg_path)\n",
    "        explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
    "                                                batch_predict, # classification function\n",
    "                                                top_labels=1, \n",
    "                                                hide_color=0, \n",
    "                                                num_samples=1000) # number of images that will be sent to classification function\n",
    "        temp1, mask1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=11, hide_rest=True)\n",
    "        img_boundry1 = mark_boundaries(temp1/255.0, mask1)\n",
    "        tmp_exp1_res = exp1.cal_val_from_lime_result(img_boundry1, xml_path=xml_path)\n",
    "        tmp_exp1_res.append(jpg_path[12:])\n",
    "        exp1_res.append(tmp_exp1_res)\n",
    "        #count += 1\n",
    "        #print(count)\n",
    "        #plt.imsave(\"reslut_pet_dataset/exp1/lime/not_hide_rest/\" + jpg_name, img_boundry1)\n",
    "    except Exception as e:\n",
    "        slack.notify(text=\"えらー\")\n",
    "        print(jpg_path[12:])\n",
    "        print(e)\n",
    "t2 = time.time()\n",
    "td = t2 - t1\n",
    "slack.notify(text=\"しゅーりょー\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"reslut_pet_dataset/exp1/lime/lime_result.txt\"\n",
    "with open(path, mode=\"r\") as f:\n",
    "    tmp = f.readlines()\n",
    "tmp = tmp[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = \"\".join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tmp.replace(\"\\n\", \" \")\n",
    "s = s.replace(\"[\", \" \")\n",
    "s = s.replace(\"]\", \" \")\n",
    "s = s.replace(\",\", \" \")\n",
    "s = s.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_path = \"reslut_pet_dataset/exp1/dbscan_and_kmeans/300/result3/ok_files_name.txt\"\n",
    "ok_files = set()\n",
    "with open(ok_path, mode=\"r\") as f:\n",
    "    for ss in f:\n",
    "        tmp = ss.split()[0][12:]\n",
    "        ok_files.add(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[326544.0, 216313.0, 326544.0, 333926.0]\n"
     ]
    }
   ],
   "source": [
    "res = [0, 0, 0, 0]\n",
    "for i in range(len(s)):\n",
    "    if i % 7 == 0:\n",
    "        if s[i + 6][1:-1] in ok_files:\n",
    "            res[0] += float(s[i])\n",
    "            res[1] += float(s[i + 1])\n",
    "            res[2] += float(s[i + 3])\n",
    "            res[3] += float(s[i + 4])\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 26 2022, 03:28:14) [GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
